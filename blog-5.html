<!DOCTYPE html>
<html lang="en">
<head>
 <title>Natural Language Processing Text Generation using Markov Chains</title>
 <!-- Latest compiled and minified CSS -->
 <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous">
 <div class="container">
  <h1><a href="https://codykane.github.io/blog">Blog</a></h1>
 </div>
</head>
<body>
 <div class="container">
<div class="row">
 <div class="col-md-8">
  <h3>Natural Language Processing Text Generation using Markov Chains</h3>
  <label>2019-08-12</label>
  <p>In this blog I'm going to use Markov chains to generate pseudo tweets from a cleaned dataset of tweets from a particular account, in my case Kanye West, but it could be any active Twitter account. If you're wondering how to get hte tweets and how to process them, my previous blog goes into detail, and it's what I am continuing from here. </p>
<div class="highlight"><pre><span></span><span class="c1">##Here I&#39;ll bring in the dataframe of cleaned tweets, </span>
<span class="c1">##then quickly convert it into a list after dropping the indexes and nans </span>
<span class="c1">#(someone some blanks were not dropped after the cleaning but it&#39;s an easy fix), then into string of strings</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="n">tweets_list</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/Users/cody/Documents/hydra/blog/tweets.csv&#39;</span><span class="p">)</span>


<span class="c1"># tweets_list = [i for i in tweets_list.text]</span>
<span class="c1"># tweets_str = &quot; &quot;.join(tweets_list)</span>
<span class="n">tweets_list</span><span class="o">=</span> <span class="n">tweets_list</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;index&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tweets_list</span> <span class="o">=</span> <span class="n">tweets_list</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Unnamed: 0&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tweets_list</span> <span class="o">=</span> <span class="n">tweets_list</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">how</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">tweets_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tweets_list</span><span class="o">.</span><span class="n">text</span><span class="p">]</span>
<span class="n">tweets_str</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tweets_list</span><span class="p">)</span>
</pre></div>


<p>Markov chains are probably the most simple form of text generation, but this can be to its advantage.  I thought of doing a deep learning algorithm on this dataset, and still may later just to see what it spits out, but with just over 750 tweets, there isn't enough data to make it worthwhile. The Markov Chain algorithm is known as being 'memory-less' because it breaks down every sentence into words, then makes a rolodex of what that word could be followed by. For example 'The' might have "cat, mouse, green" in its rolodex, and cat might have "snarled, roared, bit' in it's rolodex. Weighting the probability of each occurence, it might generate "The cat roared" if the original text corpus contained "My cat roared" and "The cat bit me". This allows it to generate sequences word by word without needing to remember deep patterns like in keras. Let's look at the data for a better idea.</p>
<div class="highlight"><pre><span></span><span class="c1">##turning the lsit into a string of strings so it is digestable to the Markov chain,</span>
<span class="c1">##then creating the Markov chain function</span>



<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

<span class="k">def</span> <span class="nf">markov_chain</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>

    <span class="c1">#Tokenize words/punctuation</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
    <span class="c1">#Make dict to hold all these split tokens </span>
    <span class="n">m_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="c1">#zipped list of word pairs </span>
    <span class="k">for</span> <span class="n">current_word</span><span class="p">,</span> <span class="n">next_word</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">words</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="n">m_dict</span><span class="p">[</span><span class="n">current_word</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_word</span><span class="p">)</span>
    <span class="c1">#Convert the default dict into a regular dict</span>
    <span class="n">m_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">m_dict</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m_dict</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1">##Putting the tweets into the markov chain we can see the potential paths the words will follow. </span>
<span class="n">ye_dict</span> <span class="o">=</span> <span class="n">markov_chain</span><span class="p">(</span><span class="n">tweets_str</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span> 

<span class="k">def</span> <span class="nf">generate_sentence</span><span class="p">(</span><span class="n">chain</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>

    <span class="c1">#Generate first word randomly, assign it to a tweet</span>
    <span class="n">word1</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">chain</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
    <span class="n">tweet</span> <span class="o">=</span> <span class="n">word1</span><span class="o">.</span><span class="n">capitalize</span><span class="p">()</span>

    <span class="c1">#Generate second word based on first, add it to the tweet</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">word2</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="n">word1</span><span class="p">])</span>
        <span class="n">word1</span> <span class="o">=</span> <span class="n">word2</span> 
        <span class="n">tweet</span> <span class="o">+=</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">word2</span>

    <span class="k">return</span> <span class="n">tweet</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">generate_sentence</span><span class="p">(</span><span class="n">ye_dict</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>&#39;Heard Hideo Kojima is important&#39;
</pre></div>


<p>Using the generate_sentence function we have generated a fake tweet! The count in the function can be changed for as many (or few) words you want. I decided against stripping the tweets of all the extra spaces, as that's how Kanye tweets, but there are lots of small things that can be tweaked. Overall I find it to generate pretty funny and plausible tweets. Try it on your favourite twitter account!</p>
 </div>
</div>
 </div>
</body>
</html>